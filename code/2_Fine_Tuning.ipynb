{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Intro\n",
    "\n",
    "The Sentiment Analysis process needs the following steps:\n",
    "\n",
    "- Feature extraction\n",
    "- Modeling\n",
    "\n",
    "#### Feature Extraction\n",
    "\n",
    "This phase is based on statistical bag-of-words methods (CountVectorizer, TfidfVectorizer)\n",
    "\n",
    "#### Modeling\n",
    "\n",
    "The models involved are some of Traditional Machine Learning ones: DecisionTree, RandomForest, GradientBoosting\n",
    "\n",
    "\n",
    "Both these points entail some parameters. In order to maximize results some of these parameters are going to be tuned. This operation represents the core of this notebook \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Packages & Basic Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import timedelta\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from ast import literal_eval\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import EvalClfPipeParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "test_size = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines_instructions = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df_filename = 'df_preprocessed.parquet'\n",
    "\n",
    "df = pd.read_parquet(os.path.join(data_path, 'intermediate', preprocessed_df_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_test_size = 0.25\n",
    "\n",
    "split_point = int(round(len(df)*(1-overall_test_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.iloc[:split_point].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluate Parameters\n",
    "\n",
    "Vectorizers hyperparameters:\n",
    "- ngram range -> token min and max length in terms of number of words\n",
    "- max features -> number of tokens (most frequent) to keep as features\n",
    "\n",
    "Models hyperparameters:\n",
    "\n",
    "DecisionTree: [max_depth, min_samples_leaf]\n",
    "\n",
    "RandomForest: [n_estimators, max_depth, max_features]\n",
    "\n",
    "GB: [n_estimators, max_depth, learning_rate]\n",
    "\n",
    "\n",
    "#### Evaluation method: GridSearch\n",
    "#### Evaluation metrics: Matthews correlation coefficient (MCC), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers = ['cv','tfidf']\n",
    "\n",
    "vectorizers_grid = {'ngram_range':[(1,3), (1,4)],'max_features':[2**9, 2**10]}\n",
    "\n",
    "models_grids = {'DecisionTreeClassifier': {'fixed':{'random_state':seed}, 'tuning':{'max_depth':[3,4], 'min_samples_leaf':[2**4,2**5]}},\n",
    "                'RandomForestClassifier': {'fixed':{'random_state':seed}, 'tuning':{'n_estimators':[2**6, 2**7], 'max_depth':[2,3,4], 'max_features':['sqrt','log2']}},\n",
    "                'GradientBoostingClassifier': {'fixed':{'random_state':seed}, 'tuning':{'n_estimators':[2**6, 2**7], 'max_depth':[2,3,4], 'learning_rate':[0.01,0.05]}}}\n",
    "\n",
    "\n",
    "\n",
    "cv_test_size = 300\n",
    "\n",
    "n_cv = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_process_eval = EvalClfPipeParams(vectorizers=vectorizers,\n",
    "                 vects_grid=vectorizers_grid,\n",
    "                 models_grid=models_grids,\n",
    "                 text=df_train['clean_bow'].tolist(),\n",
    "                 y=df_train['binary_label'].tolist(),\n",
    "                 test_size=cv_test_size,\n",
    "                 cv=n_cv)\n",
    "\n",
    "binary_process_eval.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ternary_process_eval = EvalClfPipeParams(vectorizers=vectorizers,\n",
    "                 vects_grid=vectorizers_grid,\n",
    "                 models_grid=models_grids,\n",
    "                 text=df_train['clean_bow'].tolist(),\n",
    "                 y=df_train['ternary_label'].tolist(),\n",
    "                 test_size=cv_test_size,\n",
    "                 cv=n_cv)\n",
    "\n",
    "ternary_process_eval.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Parameters selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_l = [('binary', binary_process_eval), ('ternary',ternary_process_eval)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Vectorizers parameters\n",
    "\n",
    "The fetaure extractors hyperparameters are choosen based on the ones with the best average MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf, pipe_eval in eval_l:\n",
    "\n",
    "    pipelines_instructions[clf] = {'FeatureExtraction':{}, 'Models': {}}\n",
    "\n",
    "    df_pipelines_eval = pipe_eval.results.copy()\n",
    "\n",
    "    df_pipelines_eval['v_par_str'] = df_pipelines_eval['Vect_parameters'].astype('str')\n",
    "\n",
    "    by_fe = df_pipelines_eval.groupby(['Vectorizer','v_par_str'], as_index=False).agg({'MCC':'mean','accuracy':'mean'}).sort_values(['MCC','accuracy'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "    for vect in vectorizers:\n",
    "\n",
    "        pipelines_instructions[clf]['FeatureExtraction'][vect] = literal_eval(by_fe[by_fe['Vectorizer']==vect]['v_par_str'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Models parameters\n",
    "\n",
    "The selected models hyperparameters are the ones with highest score (MCC) using the predefined (picked the cell above) vectorizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf, pipe_eval in eval_l:\n",
    "\n",
    "    df_pipelines_eval = pipe_eval.results.copy()\n",
    "\n",
    "    for vect in vectorizers:\n",
    "\n",
    "        selected_fe = pipelines_instructions[clf]['FeatureExtraction'][vect]\n",
    "\n",
    "        pipelines_instructions[clf]['Models'][vect] = {}\n",
    "\n",
    "        for model in df_pipelines_eval['Model'].unique():\n",
    "\n",
    "            pipelines_instructions[clf]['Models'][vect][model] = df_pipelines_eval.loc[(df_pipelines_eval['Vect_parameters']==selected_fe) & \\\n",
    "                                                                                     (df_pipelines_eval['Model']==model) , 'Model_parameters'].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"binary\": {\n",
      "      \"FeatureExtraction\": {\n",
      "         \"cv\": {\n",
      "            \"ngram_range\": [\n",
      "               1,\n",
      "               4\n",
      "            ],\n",
      "            \"max_features\": 1024\n",
      "         },\n",
      "         \"tfidf\": {\n",
      "            \"ngram_range\": [\n",
      "               1,\n",
      "               4\n",
      "            ],\n",
      "            \"max_features\": 1024\n",
      "         }\n",
      "      },\n",
      "      \"Models\": {\n",
      "         \"cv\": {\n",
      "            \"GradientBoostingClassifier\": {\n",
      "               \"n_estimators\": 128,\n",
      "               \"max_depth\": 2,\n",
      "               \"learning_rate\": 0.05,\n",
      "               \"random_state\": 0\n",
      "            },\n",
      "            \"RandomForestClassifier\": {\n",
      "               \"n_estimators\": 128,\n",
      "               \"max_depth\": 4,\n",
      "               \"max_features\": \"sqrt\",\n",
      "               \"random_state\": 0\n",
      "            },\n",
      "            \"DecisionTreeClassifier\": {\n",
      "               \"max_depth\": 4,\n",
      "               \"min_samples_leaf\": 32,\n",
      "               \"random_state\": 0\n",
      "            }\n",
      "         },\n",
      "         \"tfidf\": {\n",
      "            \"GradientBoostingClassifier\": {\n",
      "               \"n_estimators\": 128,\n",
      "               \"max_depth\": 2,\n",
      "               \"learning_rate\": 0.05,\n",
      "               \"random_state\": 0\n",
      "            },\n",
      "            \"RandomForestClassifier\": {\n",
      "               \"n_estimators\": 128,\n",
      "               \"max_depth\": 4,\n",
      "               \"max_features\": \"sqrt\",\n",
      "               \"random_state\": 0\n",
      "            },\n",
      "            \"DecisionTreeClassifier\": {\n",
      "               \"max_depth\": 4,\n",
      "               \"min_samples_leaf\": 32,\n",
      "               \"random_state\": 0\n",
      "            }\n",
      "         }\n",
      "      }\n",
      "   },\n",
      "   \"ternary\": {\n",
      "      \"FeatureExtraction\": {\n",
      "         \"cv\": {\n",
      "            \"ngram_range\": [\n",
      "               1,\n",
      "               4\n",
      "            ],\n",
      "            \"max_features\": 1024\n",
      "         },\n",
      "         \"tfidf\": {\n",
      "            \"ngram_range\": [\n",
      "               1,\n",
      "               4\n",
      "            ],\n",
      "            \"max_features\": 1024\n",
      "         }\n",
      "      },\n",
      "      \"Models\": {\n",
      "         \"cv\": {\n",
      "            \"GradientBoostingClassifier\": {\n",
      "               \"n_estimators\": 128,\n",
      "               \"max_depth\": 2,\n",
      "               \"learning_rate\": 0.05,\n",
      "               \"random_state\": 0\n",
      "            },\n",
      "            \"RandomForestClassifier\": {\n",
      "               \"n_estimators\": 128,\n",
      "               \"max_depth\": 4,\n",
      "               \"max_features\": \"sqrt\",\n",
      "               \"random_state\": 0\n",
      "            },\n",
      "            \"DecisionTreeClassifier\": {\n",
      "               \"max_depth\": 4,\n",
      "               \"min_samples_leaf\": 32,\n",
      "               \"random_state\": 0\n",
      "            }\n",
      "         },\n",
      "         \"tfidf\": {\n",
      "            \"GradientBoostingClassifier\": {\n",
      "               \"n_estimators\": 128,\n",
      "               \"max_depth\": 2,\n",
      "               \"learning_rate\": 0.05,\n",
      "               \"random_state\": 0\n",
      "            },\n",
      "            \"RandomForestClassifier\": {\n",
      "               \"n_estimators\": 128,\n",
      "               \"max_depth\": 4,\n",
      "               \"max_features\": \"sqrt\",\n",
      "               \"random_state\": 0\n",
      "            },\n",
      "            \"DecisionTreeClassifier\": {\n",
      "               \"max_depth\": 4,\n",
      "               \"min_samples_leaf\": 32,\n",
      "               \"random_state\": 0\n",
      "            }\n",
      "         }\n",
      "      }\n",
      "   }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(pipelines_instructions, indent=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pipelines_instructions, open(os.path.join(data_path, 'output', 'NLP_FSA_pipelines_instructions.pkl'), 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
