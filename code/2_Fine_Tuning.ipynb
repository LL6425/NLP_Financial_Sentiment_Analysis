{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Intro\n",
    "\n",
    "The Sentiment Analysis process needs the following steps:\n",
    "\n",
    "- Feature extraction\n",
    "- Modeling\n",
    "\n",
    "#### Feature Extraction\n",
    "\n",
    "This phase is based on statistical bag-of-words methods (CountVectorizer, TfidfVectorizer)\n",
    "\n",
    "#### Modeling\n",
    "\n",
    "The models involved are Traditional Machine Learning ones: DecisionTree, RandomForest, GradientBoosting\n",
    "\n",
    "\n",
    "Both these points entail some parameters. In order to maximize results some of these parameters are going to be tuned. This operation represents the core of this notebook \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Packages & Basic Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import make_scorer, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import NlpPipeBayesSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df_filename = 'df_preprocessed.parquet'\n",
    "\n",
    "df = pd.read_parquet(os.path.join(data_path, 'intermediate', preprocessed_df_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_test_size = 0.2\n",
    "\n",
    "split_point = int(round(len(df)*(1-overall_test_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df['clean_bow'].values[:split_point]\n",
    "y_bin_train=df['binary_label'].values[:split_point]\n",
    "y_ter_train=df['binary_label'].values[:split_point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Parameters Search\n",
    "\n",
    "Vectorizers hyperparameters:\n",
    "- max features -> number of tokens (most frequent) to keep as features \n",
    "\n",
    "Models hyperparameters:\n",
    "\n",
    "DecisionTree: [max_depth, min_samples_leaf]\n",
    "\n",
    "RandomForest: [n_estimators, max_depth, max_features]\n",
    "\n",
    "GB: [max_iter, max_depth, learning_rate]\n",
    "\n",
    "\n",
    "#### Evaluation method: BayesSearch\n",
    "#### Evaluation metric: Matthews correlation coefficient (MCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Search params initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizers search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vects_space = {'cv':{'object':CountVectorizer(ngram_range=(1,4)), 'space':{'max_features':(2**8,2**11)}},\n",
    "        'tfidf':{'object':TfidfVectorizer(ngram_range=(1,4)), 'space':{'max_features':(2**8,2**11)}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_space = {'DT':{'object':DecisionTreeClassifier(random_state=seed), 'space':{'max_depth':(2,4), 'min_samples_leaf':(2**3, 2**5)}},\n",
    "    'RF':{'object':RandomForestClassifier(random_state=seed), 'space':{'max_depth':(2,4), 'n_estimators':(2**5, 2**8), 'max_features':(1/2**6,1/2**2)}},\n",
    "    'HGB':{'object':HistGradientBoostingClassifier(random_state=seed), 'space':{'max_depth':(2,4), 'max_iter':(2**5, 2**8), 'learning_rate':(1/2**9,1/2**3)}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc = make_scorer(matthews_corrcoef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CV object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cv = 4\n",
    "cv_test_size = 250\n",
    "\n",
    "ts_split = TimeSeriesSplit(n_splits=n_cv, test_size=cv_test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nj = os.cpu_count() - 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Bayes Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dff00ceecb04df69e06bde54a435ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "binary_pipe_bay_search = NlpPipeBayesSearch(vects_dict=vects_space,\n",
    "                                     clfs_dict=models_space,\n",
    "                                     cv_object=ts_split,\n",
    "                                     n_iter=n_iterations,\n",
    "                                     random_state=seed,\n",
    "                                     n_jobs=nj,\n",
    "                                     scoring=mcc,\n",
    "                                     std_penalty=True)\n",
    "\n",
    "\n",
    "binary_pipe_bay_search.search(X_train, y_bin_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ac1ef5f1da4a5299f547a4d61d733d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ternary_pipe_bay_search = NlpPipeBayesSearch(vects_dict=vects_space,\n",
    "                                     clfs_dict=models_space,\n",
    "                                     cv_object=ts_split,\n",
    "                                     n_iter=n_iterations,\n",
    "                                     random_state=seed,\n",
    "                                     n_jobs=nj,\n",
    "                                     scoring=mcc,\n",
    "                                     std_penalty=True)\n",
    "\n",
    "\n",
    "ternary_pipe_bay_search.search(X_train, y_ter_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Parameters selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines_instructions = {}\n",
    "\n",
    "pipelines_instructions['binary'] = binary_pipe_bay_search.pipelines_instructions\n",
    "\n",
    "pipelines_instructions['ternary'] = ternary_pipe_bay_search.pipelines_instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pipelines_instructions, open(os.path.join(data_path, 'output', 'NLP_FSA_pipelines_instructions.pkl'), 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
